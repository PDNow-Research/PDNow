{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Images.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvh7N+fBy8YG22GS7G27ef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PDNow-Research/PDNow/blob/main/HandPD/NeuralNets/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkGertFcjmEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b1f862-edc8-4e49-c82a-e7e103253760"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9pOrBx_4cC"
      },
      "source": [
        "We are going to take text-based features and apply a simple, fully-connected neural network to them. Atually, let's use images - that's what the tutorial does. Let's only consider Meander for the time being."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3eF1D-LjzNr"
      },
      "source": [
        "# General\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "# Image Reading\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "# Other ML Preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Torch General\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torch.nn as nn # loss functions, neural network type (convolutional, linear, etc.)\n",
        "import torch.optim as optim # optimization functions (sgd)\n",
        "import torch.nn.functional as F # functions without parameters - activation functions (Relu, etc.) (also included in nn package, could use, but functional package is \"better\")\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets # torch has a LOT LOT LOT of standard datasets (ImageNet, MNIST, etc.)\n",
        "import torchvision.transforms as transforms # transformations for dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler # PyTorch train test split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7oC7YSPkdtG"
      },
      "source": [
        "## Set Device + Init Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3nthTPetEQf"
      },
      "source": [
        "# Hyperparams\n",
        "input_size = 621432 \n",
        "# This 621432 comes from the size of each image (maxSize of an image). They have all been padded to be of this size. \n",
        "# We removed color channels to decrease size.\n",
        "num_classes = 2\n",
        "\n",
        "# tunables\n",
        "test_size = 0.2\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "num_epochs = 100\n",
        "seed = 0 # just random state to start at"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajJcm8-PNalT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350d3493-da1d-4c3d-adb3-1e529a390c3b"
      },
      "source": [
        "# Device set \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # google colab provides cuda gpu\n",
        "print (device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FL-1eHknDM"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG5gu2CV-kn8"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QSHyNkXuZAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cdc20a64-f072-4f5d-daeb-4e8062da04ac"
      },
      "source": [
        "# Load Data (possible to do it with Google Drive rather than uploading from computer?).\n",
        "# Uploaded can be nice tho... especially because it automatically goes to dictionary... that can be fed into NN, with file names as indexes - quite nice\n",
        "\n",
        "\"\"\" from google.colab import files\n",
        "uploaded = files.upload()\"\"\"\n",
        "\n",
        "# %cd \"/content/drive/\"My Drive\"/Data/Images/Meander/\"\n",
        "# X_train = np.load('HealthyMeander/HealthyMeander/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' from google.colab import files\\nuploaded = files.upload()'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUoREyiB1uh8"
      },
      "source": [
        "# Healthy: Class 0 . PD: Class 1\n",
        "\n",
        "def extract_images(path, c): # path of data, class of data\n",
        "  filename_arr = []\n",
        "  X_arr = []\n",
        "\n",
        "  for file in glob.glob(path):\n",
        "    filename_arr.append(file) # filenames, not going to use them for now. Might need them later.\n",
        "    x = cv2.imread(file)\n",
        "    x = x[:,:,0] # removing all color channels to make data smaller and possible to use\n",
        "    X_arr.append(x)\n",
        "\n",
        "  y_arr = [c] * len(X_arr)\n",
        "\n",
        "  return X_arr, y_arr\n",
        "\n",
        "# X_arr is a list of 3D arrays representing images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocDBvBlTGSuf"
      },
      "source": [
        "# possibly try different pad modes for better NN results? (instead of 'constant')\n",
        "def pad_images(arr):\n",
        "  arr = np.copy(arr)\n",
        "  largestX = 0\n",
        "  largestY = 0\n",
        "\n",
        "  def pad_condition(pad, largest, index):\n",
        "    if (2 * pad != (largest - arr[i].shape[index])):\n",
        "      pad1 = pad + 1\n",
        "      pad2 = pad\n",
        "    else:\n",
        "      pad1, pad2 = pad, pad\n",
        "    return pad1, pad2\n",
        "\n",
        "  for i in arr:\n",
        "    X = i.shape[0]\n",
        "    Y = i.shape[1]\n",
        "    if (X > largestX):\n",
        "      largestX = X\n",
        "    if (Y > largestY):\n",
        "      largestY = Y\n",
        "\n",
        "  for i in range(len(arr)):\n",
        "    X_pad = int((largestX - arr[i].shape[0]) /2) # pad equally in both directions, must be int\n",
        "    Y_pad = int((largestY - arr[i].shape[1]) /2)\n",
        "    \n",
        "    # but int floors, so we might get something of a slightly wrong shape (by 1), so...\n",
        "    X_pad1, X_pad2 = pad_condition(X_pad, largestX, 0)\n",
        "    Y_pad1, Y_pad2 = pad_condition(Y_pad, largestY, 1)\n",
        "\n",
        "    arr[i] = np.pad(arr[i], ((X_pad1, X_pad2), (Y_pad1, Y_pad2)), 'constant', constant_values=(0)) # each image in arr in 2D\n",
        "\n",
        "  maxSize = largestX * largestY\n",
        "  \n",
        "  return arr, maxSize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgoIIjwgRJWk"
      },
      "source": [
        "def normalize_images(arr, num):\n",
        "  arr = arr/num\n",
        "  return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VaW62V2UM-en",
        "outputId": "e322193f-88d9-448c-899a-7c3ae7e02be4"
      },
      "source": [
        "\"\"\" X_meah, y_meah = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/HealthyMeander/HealthyMeander/*.*\",0)\n",
        "X_meap, y_meap = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/PatientMeander/PatientMeander/*.*\",1)\n",
        "\n",
        "X_all = X_meah + X_meap\n",
        "X_all, maxSize = pad_images(X_all)\n",
        "\n",
        "X_all = normalize_images(X_all, 255)\n",
        "\n",
        "y_all = np.array(y_meah + y_meap)\n",
        "\n",
        "# Why the warning particularly? \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' X_meah, y_meah = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/HealthyMeander/HealthyMeander/*.*\",0)\\nX_meap, y_meap = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/PatientMeander/PatientMeander/*.*\",1)\\n\\nX_all = X_meah + X_meap\\nX_all, maxSize = pad_images(X_all)\\n\\nX_all = normalize_images(X_all, 255)\\n\\ny_all = np.array(y_meah + y_meap)\\n\\n# Why the warning particularly? '"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeTuX0ihosiD"
      },
      "source": [
        "class MeanderDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    X_meah, y_meah = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/HealthyMeander/HealthyMeander/*.*\",0)\n",
        "    X_meap, y_meap = extract_images(\"/content/drive/MyDrive/Data/Images/Meander/PatientMeander/PatientMeander/*.*\",1)\n",
        "\n",
        "    X_spih, y_spih = extract_images(\"/content/drive/MyDrive/Data/Images/Spiral/HealthySpiral/HealthySpiral/*.*\",0)\n",
        "    X_spip, y_spip = extract_images(\"/content/drive/MyDrive/Data/Images/Spiral/PatientSpiral/PatientSpiral/*.*\",1)\n",
        "\n",
        "\n",
        "    y_all = np.array(y_meah + y_meap + y_spih + y_spip)\n",
        "\n",
        "    X_all = X_meah + X_meap + X_spih + X_spip\n",
        "    X_all, maxSize = pad_images(X_all) # just padding all of the array's images to be the same size\n",
        "    X_all = normalize_images(X_all, 255) # normalizing from 0 to 255 - 0 to 1\n",
        "\n",
        "    X_all = np.stack(X_all, axis=0) # stacking into 4D tensor\n",
        "\n",
        "    self.n_samples = X_all.shape[0]\n",
        "\n",
        "    self.x = torch.from_numpy(X_all).float() # creates tensor from numpy array, making it float as expected by model\n",
        "    self.y = torch.from_numpy(y_all).long() # y_all is numpy array too, making it a long as expected by model\n",
        "    self.inputSize = maxSize\n",
        "  \n",
        "  # support indexing such that dataset[i] can be used to get i-th sample\n",
        "  def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "  # to return size\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5rlj-zaatPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e57e5b-c727-4747-92a7-4aa013ab03cf"
      },
      "source": [
        "dataset = MeanderDataset() # Meander Dataset object\n",
        "\n",
        "# Why the warning?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqUTkoL3gaM7",
        "outputId": "f13c9515-9b20-4cb3-e8a9-539c66fed38d"
      },
      "source": [
        "dataset.inputSize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "621432"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOCtKHuXv33-",
        "outputId": "3f53a77c-b9ce-4511-9b82-54dbdd3abfbc"
      },
      "source": [
        "first_row = dataset[0]\n",
        "feature0, label0 = first_row\n",
        "print(feature0, label0)\n",
        "\n",
        "# feature0 shape is 744, 822, 3\n",
        "# label0 is just 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy4NmhAIIwV-",
        "outputId": "31deb627-986a-412d-919a-07b75f451aa4"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzYiS7yNNxBQ"
      },
      "source": [
        "### DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AYxhzwoMiga"
      },
      "source": [
        "# Dataloader to load whole dataset\n",
        "# Shuffle: shuffle data, good for training\n",
        "# num_workers: faster loading with multiple subprocesses simultaneously, set to 0 if error occurs when loading"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vfo8yMzBh_b"
      },
      "source": [
        "# To perform train test split, we'll use sklearn... https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
        "\n",
        "# generate indices: instead of the actual data we pass in integers instead\n",
        "train_indices, test_indices, _ , _ = train_test_split(\n",
        "    range(len(dataset)),\n",
        "    dataset.y,\n",
        "    stratify=dataset.y,\n",
        "    test_size=test_size,\n",
        "    random_state=seed\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk9e_kh9JPcR"
      },
      "source": [
        "# train_indices is indices of the training values while test_indices is indicies of the testing values. Let's split our data like such.\n",
        "\n",
        "# generate subset based on indices\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "test_dataset = Subset(dataset, test_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0NNckzyJgiR",
        "outputId": "72d497af-2719-4892-863a-c0571f74da7a"
      },
      "source": [
        "len(train_dataset), len(test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(422, 106)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djy29psB1Z8L"
      },
      "source": [
        "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True, num_workers =2)\n",
        "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True, num_workers =2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMbr3JjulNJI"
      },
      "source": [
        "## Defining the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRGjWjPnA2w3"
      },
      "source": [
        "Now to create the fully connected network.\n",
        "\n",
        "Unfortunately, there is a limit to the model's performance. More nodes would have been helpful, but we don't have those resources. We need to cut down on the model images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_16mfP3JkKp9"
      },
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, output_size): # input-size = 611568 (size of our images, pixel number), num_classes = 2 (PD/no PD)\n",
        "    super(NN, self).__init__() # initializes the NN class that we're defining\n",
        "\n",
        "    \"\"\"self.fc1 = nn.Linear(input_size, 750) # 50 nodes\n",
        "    self.fc2 = nn.Linear(, num_classes)\"\"\"\n",
        "\n",
        "    self.fc1 = nn.Linear(input_size, 750)\n",
        "    self.fc2 = nn.Linear(750, 500) # 2 million to 500 thousand\n",
        "    self.fc3 = nn.Linear(500, 100)\n",
        "    self.dt1 = nn.Dropout(0.1, inplace=False) \n",
        "    self.fc4 = nn.Linear(100, output_size)\n",
        "\n",
        "  def forward(self, x): # run on some input x, which is the images which we run through fc1 and fc2 layers created above (and add the reLU activation function it between)\n",
        "    \"\"\"x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\"\"\"\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.sigmoid(self.fc2(x)) # functional library.sigmoid is deprecated\n",
        "    x = F.sigmoid(self.fc3(x))\n",
        "    x = self.dt1(x)\n",
        "    x = F.softmax(self.fc4(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hJ98iivMqS6"
      },
      "source": [
        "### Quick Test\n",
        "\n",
        "What the model should output it something of shape [264 (140 + 124), 2]. For each image, it should predict the probability of it being in class 1 or 2 and return both of those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JHWnwgXgg9NY",
        "outputId": "a4d9ef99-9088-4873-8dbf-d9341bcb7dfb"
      },
      "source": [
        "\"\"\" model = NN(621432, 2) \n",
        "\n",
        "# 124 patient images\n",
        "# 140 control images \n",
        "x = torch.randn(64, 621432) # 64 batch size\n",
        "\n",
        "print (model(x).shape)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' model = NN(621432, 2) \\n\\n# 124 patient images\\n# 140 control images \\nx = torch.randn(64, 621432) # 64 batch size\\n\\nprint (model(x).shape)'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTx3jhOBK8Ns"
      },
      "source": [
        "## Initialize Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP-SEtZyLAUE"
      },
      "source": [
        "model = NN(input_size = input_size, output_size = num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUS61pUBLFvk"
      },
      "source": [
        "# Loss + Optimizer\n",
        "criterion = nn.CrossEntropyLoss() # loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate) # optimizer function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO7z25t4KIWY"
      },
      "source": [
        "## Train Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "CCNdQnCYKKxk",
        "outputId": "e708409f-f491-4e6a-babf-931d43ba69b1"
      },
      "source": [
        "# epochs: number of times network sees images. 1 epoch - seen all images once\n",
        "\n",
        "loss_values = [] * 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader): # parts of the train_loader, (data,targets) in tuple together, batch_idx there before\n",
        "        \n",
        "        # Get data to cuda (that's our device, if it's possible)\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        # Get to correct shape, which is one long, unrolled, vector of size 744 * 822 * 3 = 1834704\n",
        "        data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "        # by looking at this, we see we have 4 batches: 3 of size 64, 1 of size 19. Also, our shape is right now.\n",
        "        # print (data.shape)\n",
        "\n",
        "        # forward (why called forward??) - forward propagation\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward (why called backward??) - backward propagation\n",
        "        optimizer.zero_grad() # set all gradients to 0 for each batch so it doesnt store calculation from previous batch\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_values.append(loss.item())\n",
        "\n",
        "print (loss_values)\n",
        "plt.plot(np.array(loss_values), 'r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-29e0efefc677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# backward (why called backward??) - backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set all gradients to 0 for each batch so it doesnt store calculation from previous batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# gradient descent or adam step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.74 GiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 1.42 GiB free; 9.30 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfBJOAYXNn38"
      },
      "source": [
        "## Check Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgWgMQdNpMe"
      },
      "source": [
        "def check_accuracy(loader, model):\n",
        "  \"\"\" if loader.dataset.train: # if this is true, then it is loading and checking training data\n",
        "    print ('Checking accuracy on training data')\n",
        "  else: \n",
        "    print ('Checking accuracy on testing data')\n",
        "\n",
        "  Appears to be error with this code  \n",
        "  \"\"\"\n",
        "\n",
        "  # initializing\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  y_pred = [] * 1\n",
        "  y_true = [] * 1\n",
        "\n",
        "  model.eval() # why this specifically?\n",
        "\n",
        "  with torch.no_grad(): # so we don't have to compute gradients for accuracy\n",
        "    for x, y in loader:\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device)\n",
        "      y_true.append(y)\n",
        "\n",
        "      x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "      scores = model(x)\n",
        "\n",
        "      # scores is size 264 * 2\n",
        "      # _, is don't store this part in anything\n",
        "      _, prediction = scores.max(1) # gives us index of maximum score value (max along second dimension, reason for the 1) in predictions variable each time\n",
        "      \n",
        "      y_pred.append(prediction)\n",
        "      \n",
        "      num_correct += (prediction == y).sum()\n",
        "      num_samples += prediction.size(0)\n",
        "\n",
        "  model.train()\n",
        "  acc = num_correct/num_samples\n",
        "  return acc, y_true, y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzthtBRLOh4G"
      },
      "source": [
        "# running check_accuracy on training and test set\n",
        "train_acc, _, _ = check_accuracy(train_loader, model)\n",
        "test_acc, y_test, test_pred = check_accuracy(test_loader, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJGJo9_pzgO6"
      },
      "source": [
        "print(train_acc, test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I949gadP0M-H"
      },
      "source": [
        "test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCgRvzA00LIc"
      },
      "source": [
        "test_pred = torch.cat(test_pred) # \n",
        "y_test = torch.cat(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX_2YayY0XC3"
      },
      "source": [
        "test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3iYZezjzjkP"
      },
      "source": [
        "conf_mat = confusion_matrix(y_test.cpu(), test_pred.cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phKkbRLewteO"
      },
      "source": [
        "tn, fp, fn, tp = conf_mat.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdWAfOZs0mYT"
      },
      "source": [
        "tp, fn, tn, fp"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}